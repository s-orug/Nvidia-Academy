IB is developed by InfiniBand Trade Association (IBTA)

The specification was released in 2000

**What is InfiniBand?**
InfiniBand is an Industry-standard specification that defines and input/output architecture used to interconnect servers, and infra equipment, storage, and embedded systems

**Why InfiniBand?**
- High Bandwidth
- Low latency
- Scalability / Flexibility

InfiniBand began in 2002 with a port speed of 10 Gbps. NVIDIA is currently shipping NDR 400 Gbps. IB port bandwidth is achieved through a combination of multiple aggregated physical lanes. The IB specification supports up to 12 physical lanes, and the common implementation includes 4 physical lanes.

| Physical Link Speed | Link Width | Link Rate |
| ------------------- | ---------- | --------- |
| EDR                 | 25 Gbps    | 100 Gbps  |
| HDR                 | 50 Gbps    | 200 Gbps  |
| NDR                 | 100 Gbps   | 400 Gbps  |
**InfiniBand Topologies**
Being able to support a wide variety of topologies, InfiniBand answers different customer requirements such as:
- Easy network scale-out
- Minimum latency between fabric nodes
- Reduced total cost of ownership
- Maximum distance
- Maximum blocking ratio

**InfiniBand Fabric Components**
- **Gateway/Bridge:** A device that moves packets from an InfiniBand fabric to and Ethernet network and vice versa.
- **Switch:** A device that routes packets between different nodes within the local InfiniBand subnet.
- **Host Channel Adapter (HCA):** A device that terminates an InfiniBand link, executes transport-level functions and supports the verbs interface.
- **Router:** A device that transports packets between different InfiniBand subnets.

**Simplified Management**
- Every InfiniBand fabric has its own Subnet Manager (SM) -- not human -- but rather a software program that runs and manages the fabric.
- Plug and play fabric management includes:
	1. Nodes and links discovery.
	2. Local identifier assignments (LIDs).
	3. Routing table calculations and deployments.
	4. Configuring nodes and ports parameter like Quality of Service (Qo)S policy.

**Network Scale-out**
Single InfiniBand subnet scales up to 48k nodes. To scale beyond 48k nodes, InfiniBand routers must be used.

**CPU Offloads**
- The InfiniBand architecture supports data transfer with minimal CPU intervention.
- This is achievable, thanks to:
	- Hardware-based transport protocol
	- Kernel bypass or zero copy
	- Remote Direct Memory Access (RDMA) - RDMA allows direct memory access from the memory of one node into that of another without involving either one's CPU.

**Traditional CPU Applications** - No CPU Offloads
Every packet produced by the application must pass through the socket library -> TCP/IP in the OS -> device driver -> hardware -> sent to the other side

**RDMA**
Buffer -> RDMA library (instead of the socket library) -> hardware

**GPU Direct RDMA - Off-loading compute nodes is implemented by NVIDIA GPUs as well**
GPU-Direct allows direct data transfer from the memory of one GPU to the memory of another. It enables lower latency and improved GPU computation performance

**Load-balancing**
- How to best utilize and optimize the network are critical requirements that should be addressed in any modern high performing data center
- Adaptive Routing supports dynamic load balancing, avoiding in-network congestion  and optimizing network bandwidth utilization
- Adaptive Routing is enabled on the NVIDIA's switches' hardware and managed by the Adaptive Routing Manager.

**Quality of Service**
Quality of Service (**QoS**) is the ability to provide different priority to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow.

QoS can be achieved by:
- Defining I/O channels at the adapter level
- Defining Virtual Lanes at the link level
This allows for control of the congestion on the network

**SHARP - Collective Operations Offload**
- SHARP is a mechanism based on NVIDIA's switch hardware and a central management package.
- SHARP offloads collective operations from host CPUs or GPUs to the network switches, and eliminates the need to send data multiple times between end points.
- SHARP dramatically improves the performance of accelerated computing MPI based application by up to x10.